# -*- coding: utf-8 -*-
"""AdvanceDS_Project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sDm17ZivBbBeWBpg6IBRg4vJr1Z2OxnE

# **FDA Food Event: Soft Drink/Water**

# **I. Introduction and Dataset Description**

As one of the most commonly consumed products, soft drinks/water products have far-reaching effects beyond simple weight gain. In severe cases, consumption can lead to hospitalization, disability, or even life-threatening consequences. To better understand these risks, we leverage the openFDA Food, Dietary Supplements, and Cosmetic Adverse Event API, which provides data from the Center for Food Safety and Applied Nutrition's Adverse Event Reporting System (CAERS). This database contains information on adverse event and product complaint reports submitted to the FDA from 2004 to 2025-02-16.

Our goal is to explore various product types within the soft drink and water industry, identify vulnerable groups (such as age and gender), and analyze the reactions, severity of impact, and correlation of symptoms associated with these products.

# **II. Data Acquisition**

In this section, we use openFDA's Food API to examine adverse events linked to FDA-regulated foods, specifically Soft Drink/Water industry. Due to API constraints, the maximum value for the limit parameter is set at 1000. Therefore, we retrieve the data in chunks to gather a comprehensive dataset for analysis with for-loop. For each iteration, an API request is made, resulting in a total of 3476 data records.
"""

from urllib.request import urlopen
import pandas as pd
import json

data_list = []
for i in range(0, 4000, 1000):
  url = f"https://api.fda.gov/food/event.json?search=products.industry_name:%22Soft+Drink/Water%22&skip={i}&limit=1000"
  response = urlopen(url)
  data_json = json.loads(response.read())
  data_list.extend(data_json["results"])

with open("data/raw_fda_soft_drink_data.json", "w") as file:
    json.dump(data_list, file, indent=4)

print("Data has been saved to raw_fda_soft_drink_data.json")

df = pd.DataFrame(data_list)
print("Size of the DataFrame:", df.shape)
df.head()

"""# **III. Data Cleaning and Handling Inconsistencies**

In this section, we will:

*  **Fix Incorrect Data Formats:** We will convert dates from strings to datetime format.
*  **Correct Nested Data:** The consumer and product columns contain nested dictionaries. We will separate these into individual columns for easier access.
*  **Handle List Columns:** The outcomes and reactions columns contain lists of items. We will create dummy variables for each item in the lists for more effective analysis.
*  **Remove Missing Values:** Rows with missing or null values will be removed.
*  **Eliminate Duplicate Rows:** We will remove any duplicate rows to ensure the dataset contains only unique records.
*  **Standardize Text Data:** All alphabetic characters will be converted to lowercase to maintain uniformity.
*  **Convert String to Numeric:** The age column, which is stored as a string, will be converted into a numeric format for analysis.
"""

import numpy as np

# Ensure Proper Datetime Formatting
df['date_created'] = pd.to_datetime(df['date_created'], format='%Y%m%d', errors='coerce')
df['date_started'] = pd.to_datetime(df['date_started'], format='%Y%m%d', errors='coerce')

# Normalize the 'consumer' column
consumer_df = pd.json_normalize(df['consumer'])
df = pd.concat([df.drop(columns=['consumer']), consumer_df], axis=1)

# Separate the 'role' and 'name_brand' from the 'products' column
df['product_role'] = df['products'].apply(lambda x: x[0]['role'] if isinstance(x, list) and len(x) > 0 else None)
df['product_name_brand'] = df['products'].apply(lambda x: x[0]['name_brand'].lower() if isinstance(x, list) and len(x) > 0 and isinstance(x[0].get('name_brand'), str) else None)

df.drop(columns=['products'], inplace=True)

# Drop report_number column, product_role and age_unit column since they are not necessary for further analysis
df.drop(columns=['report_number'], inplace=True)
df.drop(columns=['age_unit'], inplace=True)
df.drop(columns=['product_role'], inplace=True)

# Since data are in list format, flatten the data
df['outcomes'] = df['outcomes'].apply(lambda x: x if isinstance(x, list) else [x]) \
                             .explode() \
                             .apply(lambda x: x.lower() if isinstance(x, str) else x) \
                             .reset_index(drop=True)

df['reactions'] = df['reactions'].apply(lambda x: x if isinstance(x, list) else [x]) \
                                 .explode() \
                                 .apply(lambda x: x.lower() if isinstance(x, str) else x) \
                                 .reset_index(drop=True)

# Create dummy variables for the 'outcomes' column
outcomes_dummies = pd.get_dummies(df['outcomes'], prefix='outcome')

# Create dummy variables for the 'reactions' column
reactions_dummies = pd.get_dummies(df['reactions'], prefix='reaction')

df = pd.concat([df, outcomes_dummies, reactions_dummies], axis=1)

# Remove missing values
df = df.dropna()

# Drop duplicates
df = df.drop_duplicates()

# Convert age from string to numeric
df['age'] = pd.to_numeric(df['age'], errors='coerce')

# Reset the index for dataframe
df = df.reset_index(drop=True)

print("Size of the cleaned DataFrame:", df.shape)

df.to_csv("data/cleaned_fda_soft_drink_data.csv", index=False)

print("Cleaned data has been saved to cleaned_fda_soft_drink_data.csv")

df.head()

"""# **IV. Exploratory Data Analysis:**

We now expore the dataset by:
* Generate summary statistics and descriptive insights
* Visualize distributions, relationships, and trends
* Identify patterns, correlations, and anomalies
* Provide initial interpretations of data insights
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

dummy_col = df.columns[df.columns.str.startswith("reaction_")].tolist()
dummy_col.extend(df.columns[df.columns.str.startswith("outcome_")].tolist())
print(dummy_col)

for i in df:
  if i in dummy_col:
    df[i] = df[i].astype(int)

"""**Data Summary:**"""

summary = df.describe(include='all')
summary

"""From last summary table, we find some descriptive insights:


1. **Data Overview**: The dataset contains **1,020 records** and **530 features**, including timestamps, categorical variables, and one-hot encoded `reaction_` and `outcome_` variables.  

2. **Time Coverage**: Most records are from **2010 onward**, but `date_started` has an outlier of 1959.

3. **Demographics**: **Median age is 38.42 years**, with cases ranging from **1 to 90 years** (possible outliers). **Female cases are the most common(563 occurrences)**.  

4. **Product Insights**: The dataset includes **763 unique brands**, with `diet coke` appearing most frequently (35 times).  

5. **Common Reactions**: `diarrhoea` is the most frequently reported reaction (86 cases), while `reaction_vomiting` occurs relatively often (mean = 0.082).  

6. **Severe Outcomes**: **Death is recorded in 0.5% of cases**, and **allergic reactions occur in 0.1%**, indicating some high-risk cases in the dataset.

**Age Group Distribution:**
"""

df['age'] = pd.to_numeric(df['age'], errors='coerce')

bins = [0, 18, 30, 40, 50, 60, 70, 80, 100]
labels = ['<18', '18-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80+']
df['age_group'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)

plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='age_group', order=labels, palette="viridis")
plt.title('Age Group Distribution')
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.savefig('EDA_figures/age_group_distribution.png')
plt.show()


"""It is easily found that:

*   The distribution of the population is mainly concentrated in the two intervals of <18 and 40-49.
*   It declines sharply after the age of 50-59 years.

**Anomaly analysis:**
"""

plt.figure(figsize=(12, 6))

# Create subplots
plt.subplot(1, 2, 1)
sns.boxplot(x=df['age'], color="lightcoral")
plt.title("Distribution of Age")
plt.xlabel("Age")
plt.grid(True)

plt.subplot(1, 2, 2)
sns.boxplot(x=df['date_started'].map(pd.Timestamp.toordinal), color="skyblue")
plt.title("Distribution of Date Started")
plt.xlabel("Date Started")
plt.grid(True)

# Show the plots
plt.tight_layout()
plt.savefig('EDA_figures/distribution_age_and_date_started.png')
plt.show()


"""**Initial Insights from Boxplots:**

1, Age Distribution:

*   Most cases are from adults (20-60 years old).
*   Some outliers exist, including very young (0 years) and very old (80+ years) cases.


2, Date Started Distribution:

*   Most reports are from 2000 onwards, with a peak around 2010-2020.

*   few very old cases (1960s-1990s) may be historical records or data errors.

**Top 10 reported Brands:**
"""

Top10_food = df['product_name_brand'].value_counts().head(10)

plt.figure(figsize=(8,5))
top10_foodp = Top10_food.plot(kind='barh', title="Top 10 Reported Brands")
for index, value in enumerate(Top10_food):
    top10_foodp.text(value + 0.5, index, str(int(value)), va='center')
plt.savefig('EDA_figures/top_10_reported_brands.png')
plt.show()


"""Diet Coke is showing a faulty lead."""

df['year_created'] = df['date_created'].dt.year
product_trend = df.groupby(['year_created', 'product_name_brand']).size().reset_index(name='count')
top_products = df['product_name_brand'].value_counts().nlargest(10).index

product_trend_filtered = product_trend[product_trend['product_name_brand'].isin(top_products)]
plt.figure(figsize=(12, 6))
sns.lineplot(data=product_trend_filtered, x='year_created', y='count', hue='product_name_brand', marker="o")

plt.title("Trend of Top 10 Reported Products Over Time")
plt.xlabel("Year")
plt.ylabel("Number of Reports")
plt.legend(title="Product Name", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.savefig('EDA_figures/trend_top_10_reported_products_over_time.png')
plt.show()


"""With the exception of heavy fluctuation of Diet Coke and Exemption 4, the rest brands are basically stable between 0 and 2 across years

**Top 10 Reactions:**
"""

top10_r = df['reactions'].value_counts().head(10)
top10_r.index = top10_r.index.str.replace('reaction_', '', regex=True)
top10_rp = top10_r.plot(kind='barh', title="Top 10 Reactions")
plt.xlabel('Reaction')
plt.ylabel('Count')
for index, value in enumerate(top10_r):
    top10_rp.text(value + 0.5, index, str(int(value)), va='center')
plt.savefig('EDA_figures/top_10_reactions.png')
plt.show()


"""We found that nausea, vomiting, and diarrhoea are the most common reactions."""

reaction_trend = df.groupby(['year_created', 'reactions']).size().reset_index(name='count')
top_reaction = df['reactions'].value_counts().nlargest(10).index

reaction_trend_filtered = reaction_trend[reaction_trend['reactions'].isin(top_reaction)]
plt.figure(figsize=(12, 6))
sns.lineplot(data=reaction_trend_filtered, x='year_created', y='count', hue='reactions', marker="o")

plt.title("Trend of Top 10 Reported reaction Over Time")
plt.xlabel("Year")
plt.ylabel("Number of Reports")
plt.legend(title="Reactions Name", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.savefig('EDA_figures/trend_top_10_reported_reaction_over_time.png')
plt.show()


"""With the exception of heavy fluctuation of nausea, diarrhoea, and vomiting, the rest reactions are basically stable between 0 and 4 across years.

**Health Outcomes Distribution:**
"""

outcome_cols = [col for col in df.columns if 'outcome_' in col]
outcome_counts = df[outcome_cols].sum().sort_values(ascending=False)
outcome_counts.index = outcome_counts.index.str.replace('outcome_', '', regex=True)
plt.figure(figsize=(8,5))
outcome_countsp = outcome_counts.plot(kind='barh', title="Health Outcomes Distribution")
for index, value in enumerate(outcome_counts):
    outcome_countsp.text(value + 0.5, index, str(int(value)), va='center')
plt.savefig('EDA_figures/health_outcomes_distribution.png')
plt.show()


"""From the bar chart of above, the least frequent outcomes include "congenital anomaly" and "allergic reaction," each with only 1 occurrence.

**Correlation Analysis:**
"""

reaction_cols = [col for col in df.columns if 'reaction_' in col]
reaction_counts = df[reaction_cols].sum().sort_values(ascending=True)
top_10_reactions = reaction_counts.sort_values(ascending=False).head(10).index
top10_columns = list(top_10_reactions)
correlation_subset = df[top10_columns].corr()

clean_reactions = [col.replace('reaction_', '') for col in top_10_reactions]
correlation_subset.index = clean_reactions
correlation_subset.columns = clean_reactions

plt.figure(figsize=(10, 6))
sns.heatmap(correlation_subset, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap (Top 10 Reactions)")
plt.savefig('EDA_figures/correlation_heatmap_top_10_reactions.png')
plt.show()


"""This heatmap shows that the absence of concurrent symptoms was demonstrated."""

selected_columns2 = list(top_10_reactions) + outcome_cols
correlation_subset2 = df[selected_columns2].corr()

correlation_subset_reformatted = correlation_subset2.loc[outcome_cols, top_10_reactions]

clean_outcomes = [col.replace('outcome_', '') for col in outcome_cols]

correlation_subset_reformatted.index = clean_outcomes
correlation_subset_reformatted.columns = clean_reactions

plt.figure(figsize=(10, 6))
sns.heatmap(correlation_subset_reformatted, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap (Top 10 Reactions & Outcomes)")
plt.xlabel("Top 10 Reactions")
plt.ylabel("Outcomes")
plt.savefig('EDA_figures/correlation_heatmap_top_10_reactions_outcomes.png')
plt.show()


"""Obvious relationship between congenital anomaly and nausea is shown, also combined with some other significant relation such as death and abdominal pain upper, we could make further research on them.

**Relationship Between Age and Gender Distribution:**
"""

sns.scatterplot(data=df, x='gender', y='age')
plt.title("age vs gender")
plt.xlabel("gender")
plt.ylabel("age")
plt.legend(title="age vs gender")
plt.savefig('EDA_figures/age_vs_gender.png')
plt.show()


"""The age distribution of male and female is a little different. There are more higher aged male than female.  Besides this, the age distribution of male is more even than female"""



"""**Time-based Feature Engineering**"""

# Calculate time difference between report and incident
df['report_delay'] = (df['date_created'] - df['date_started']).dt.days

# Extract temporal features
df['report_year'] = df['date_created'].dt.year
df['report_month'] = df['date_created'].dt.month
df['report_weekday'] = df['date_created'].dt.weekday
df['report_quarter'] = df['date_created'].dt.quarter

# Create season feature
df['season'] = df['report_month'].map({
    1: 'Winter', 2: 'Winter', 3: 'Spring',
    4: 'Spring', 5: 'Spring', 6: 'Summer',
    7: 'Summer', 8: 'Summer', 9: 'Fall',
    10: 'Fall', 11: 'Fall', 12: 'Winter'
})

# Check time-based features
print("\nTime-based Features Summary:")
print("\nReport Delay Statistics:")
print(df['report_delay'].describe())
print("\nTemporal Distribution:")
print("Year distribution:", df['report_year'].value_counts().sort_index().head())
print("Season distribution:", df['season'].value_counts())

"""**Product Feature Engineering**"""

# Extract product categories
def categorize_product(name):
    name = str(name).lower()
    if 'water' in name:
        return 'Water'
    elif any(x in name for x in ['energy', 'monster', 'red bull']):
        return 'Energy Drink'
    elif any(x in name for x in ['cola', 'pepsi', 'coke']):
        return 'Cola'
    elif 'gatorade' in name:
        return 'Sports Drink'
    else:
        return 'Other'

df['product_category'] = df['product_name_brand'].apply(categorize_product)

# Create brand feature
def extract_brand(name):
    name = str(name).lower()
    major_brands = ['coca-cola', 'pepsi', 'monster', 'gatorade', 'red bull',
                   'dasani', 'aquafina', 'poland spring']
    for brand in major_brands:
        if brand in name:
            return brand
    return 'other'

df['brand'] = df['product_name_brand'].apply(extract_brand)

# Check product features
print("\nProduct Features Summary:")
print("\nProduct Category Distribution:")
print(df['product_category'].value_counts())
print("\nBrand Distribution:")
print(df['brand'].value_counts())

"""**Age Feature Engineering**"""

# Create binary features for vulnerable age groups
df['is_minor'] = (df['age'] < 18).astype(int)
df['is_elderly'] = (df['age'] >= 65).astype(int)

# Check age features
print("\nAge Features Summary:")
print("\nAge Group Counts:")
print("Minors (< 18):", df['is_minor'].sum())
print("Elderly (>= 65):", df['is_elderly'].sum())
print("\nAge Statistics by Category:")
print(df.groupby('product_category')['age'].describe())

"""**Reaction Severity Score**"""

# Calculate reaction count
reaction_cols = [col for col in df.columns if col.startswith('reaction_')]
df['reaction_count'] = df[reaction_cols].sum(axis=1)

# Create severity score
outcome_cols = [col for col in df.columns if col.startswith('outcome_')]
outcome_weights = {
    'outcome_death': 5,
    'outcome_life threatening': 4,
    'outcome_hospitalization': 3,
    'outcome_disability': 3,
    'outcome_required intervention': 2,
    'outcome_visited emergency room': 2,
    'outcome_visited a health care provider': 1,
    'outcome_other outcome': 1
}

df['severity_score'] = sum(df[col] * weight
                          for col, weight in outcome_weights.items()
                          if col in df.columns)

# Check severity features
print("\nSeverity Features Summary:")
print("\nReaction Count Statistics:")
print(df['reaction_count'].describe())
print("\nSeverity Score Statistics:")
print(df['severity_score'].describe())
print("\nAverage Severity by Product Category:")
print(df.groupby('product_category')['severity_score'].mean().sort_values(ascending=False))

"""**Standardize Numerical Features**"""

from sklearn.preprocessing import StandardScaler

numeric_features = ['age', 'reaction_count', 'severity_score', 'report_delay']
scaler = StandardScaler()
df[numeric_features] = scaler.fit_transform(df[numeric_features])

# Check standardized features
print("\nStandardized Features Summary:")
for feature in numeric_features:
    print(f"\n{feature} statistics after standardization:")
    print(df[feature].describe())

"""**Encode Categorical Variables**"""

categorical_features = ['season', 'product_category', 'brand', 'gender']
df_encoded = pd.get_dummies(df, columns=categorical_features, prefix=categorical_features)

# Check encoded features
print("\nEncoded Features Summary:")
print("\nOriginal shape:", df.shape)
print("Encoded shape:", df_encoded.shape)
print("\nNew encoded columns (sample):")
print(df_encoded.columns[-20:])  # Show last 20 columns as sample

"""**Create Interaction Features**"""

df_encoded['age_severity'] = df_encoded['age'] * df_encoded['severity_score']
df_encoded['reactions_delay'] = df_encoded['reaction_count'] * df_encoded['report_delay']

# Check interaction features
print("\nInteraction Features Summary:")
print("\nAge-Severity Interaction Statistics:")
print(df_encoded['age_severity'].describe())
print("\nReactions-Delay Interaction Statistics:")
print(df_encoded['reactions_delay'].describe())

# Final dataset summary
print("\nFinal Dataset Summary:")
print("Final shape:", df_encoded.shape)
print("\nMemory usage:", df_encoded.memory_usage().sum() / 1024**2, "MB")
print("\nSample of final features:")
print(df_encoded.head())
df_encoded.to_csv("data/processed_fda_soft_drink_data.csv", index=False)

"""# **VI. Challenges faced and future recommendations**

**Challenges:**


*   While obtaining the data from the FDA, there is a constraint of 1,000 reports per request, so we came up with the idea of using a for-loop for multiple requests.


**Future Recommendations:**


*   It is important to note that products like Diet Coke, which result in the most adverse events, may not necessarily be dangerous because of the product itself. Instead, it could be because it is consumed more frequently than others, leading to more adverse events. Therefore, in the future, it might be worth considering the ratio of adverse events to the number of times a product is consumed.

# **VII. Each memberâ€™s contribution**


*   **Preach Apintanapong:** Preach is responsible for finding the data source from FDA, acquiring it using API requests, and performing data cleaning processes, as well as handling the GitHub repository.
*   **Liu Yang:** Liu is responsible for conducting exploratory data analysis (EDA), creating visualizations using tools like Matplotlib and Seaborn, identifying trends and anomalies in the data, and summarizing key insights to guide further analysis.
*   **Jingxi Li:** Jingxi is responsible for conducting exploratory data analysis, exploring the data by distribution and covariance analysis, as well as helping to optimize the API acquiring process.
*   **Yemin Wang:** Yemin is responsible for data preprocessing and feature engineering, including encoding catagorical variables and creating new features

# **VIII. Link to your GitHub repository**
[https://github.com/ruangkawprc/AppliedDataScience_Project1](https://github.com/ruangkawprc/AppliedDataScience_Project1)
"""